# SentinXFL Environment Configuration
# Copy to .env and fill in values

# ============================================
# Application Settings
# ============================================
APP_NAME=SentinXFL
APP_VERSION=2.0.0
DEBUG=false
ENVIRONMENT=development  # development | staging | production

# ============================================
# API Server
# ============================================
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_RELOAD=true

# ============================================
# Security
# ============================================
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
SECRET_KEY=your-super-secret-key-change-this
JWT_ALGORITHM=HS256
JWT_EXPIRATION_MINUTES=60
CORS_ORIGINS=["http://localhost:3000"]

# ============================================
# Database (DuckDB)
# ============================================
DUCKDB_PATH=data/sentinxfl.duckdb
DUCKDB_MEMORY_LIMIT=2GB
DUCKDB_THREADS=4

# ============================================
# Data Paths
# ============================================
DATA_DIR=data/datasets
PROCESSED_DIR=data/processed
MODELS_DIR=models/checkpoints
LOGS_DIR=logs

# ============================================
# Privacy Settings
# ============================================
DP_EPSILON=1.0
DP_DELTA=1e-5
DP_MAX_GRAD_NORM=1.0
PII_AUDIT_ENABLED=true
PII_STRICT_MODE=true

# ============================================
# Federated Learning
# ============================================
FL_SERVER_ADDRESS=0.0.0.0:8080
FL_MIN_CLIENTS=2
FL_ROUNDS=10
FL_LOCAL_EPOCHS=3
FL_AGGREGATION_STRATEGY=fedavg  # fedavg | multi_krum | trimmed_mean | coordinate_median

# ============================================
# ML Settings
# ============================================
ML_BATCH_SIZE=512
ML_LEARNING_RATE=0.001
ML_TRAIN_SPLIT=0.7
ML_VAL_SPLIT=0.15
ML_TEST_SPLIT=0.15
ML_RANDOM_SEED=42

# ============================================
# LLM Settings - Provider Selection
# ============================================
# Available Providers:
#   FREE (default):
#     - ollama: Local Ollama server (recommended)
#     - local: Direct HuggingFace inference
#   PAID (scaffolded for future):
#     - openai: OpenAI API
#     - anthropic: Anthropic Claude API
#     - groq: Groq fast inference API

LLM_PROVIDER=ollama  # Using FREE local option

# Ollama Settings (FREE - https://ollama.com)
# Install: winget install Ollama.Ollama
# Run: ollama pull phi3:mini
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=phi3:mini  # Free models: phi3:mini, llama3.2:3b, mistral, qwen2.5

# Local HuggingFace Settings (FREE alternative)
LLM_MODEL_ID=microsoft/Phi-3-mini-4k-instruct
LLM_QUANTIZATION=4bit
LLM_DEVICE=cuda  # cuda | cpu

# Paid API Keys (SCAFFOLDED - uncomment when ready to upgrade)
# OPENAI_API_KEY=sk-your-openai-key
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
# GROQ_API_KEY=gsk-your-groq-key

# Common LLM Settings
LLM_MAX_NEW_TOKENS=512
LLM_TEMPERATURE=0.7

# ============================================
# ChromaDB (RAG) - FREE local vector database
# ============================================
CHROMA_PERSIST_DIR=chroma_db
CHROMA_COLLECTION_NAME=sentinxfl_docs
# FREE local embedding model (no API costs)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# ============================================
# Logging
# ============================================
LOG_LEVEL=INFO  # DEBUG | INFO | WARNING | ERROR | CRITICAL
LOG_FORMAT=json
LOG_FILE=logs/sentinxfl.log
LOG_ROTATION=10 MB
LOG_RETENTION=30 days
